{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSC160 Data Science and the Arts - Twomey - Spring 2020 - [dsc160.roberttwomey.com](http://dsc160.roberttwomey.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks as Feature Extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook shows the use of a pretrained convolutional neural network (VGG16) as a feature extractor for images. It will produce an approximately 25k dimensional feature vector to describe each image, according to the activations of the feature maps of the deep layers of the network. These features are a common approach for clustering or visualizing (dimensional reduction) image data.\n",
    "\n",
    "For background on convolutional neural networks, please see the following links: \n",
    "\n",
    "- Introduction to Image Kernels / convolution: http://setosa.io/ev/image-kernels/\n",
    "- Visualization of MNIST Digit recognition with CNN: https://www.cs.ryerson.ca/~aharley/vis/conv/flat.html\n",
    "- VGG16\n",
    "  - Simonyan and Zisserman, 'Very Deep Convolutional Networks for Large-Scale Image Recognition' (2014) [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)\n",
    "  - Web introduction to VGG16: https://neurohive.io/en/popular-networks/vgg16\n",
    "  \n",
    "We will talk more about convolutional neural networks in the coming weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install umap --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import umap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG-16 was one of the best performing architecture in [ILSVRC](http://www.image-net.org/challenges/LSVRC/) challenge 2014. It was the runner up in classification task with top-5 classification error of 7.32% (only behind GoogLeNet with classification error 6.66%). It was also the winner of localization task with 25.32% localization error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG is a deep convolutional neural network, with stacks of convolutional layers producing higher level features as you get deeper into the network. The output (fully connected) layers and softmax take those feature maps and predict image class ('sailboat', 'automobile', etc). This is the basis for a whole image classifier. \n",
    "\n",
    "During training, input images (at left) and class labels (at right, 1000 classes) are used to learn the weights of the kernels for each of these convolutional layers above. Once trained, we can also use the penultimate layers (7 x 7 x 512 above) as a feature vector describing the images in this high dimensional feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of kernels and convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2DConvUrl](https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif \"2D Conv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An image kernel is a small matrix used to apply effects like the ones you might find in Photoshop or Gimp, such as blurring, sharpening, outlining or embossing. They're also used in machine learning for 'feature extraction', a technique for determining the most important portions of an image. In this context the process is referred to more generally as \"convolution\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell loads the pretrained VGG16 (trained on imagenet), but leaves off the \"top\" of the network (the softmax and fully connected layeras). We do not want to do classfication here, we are just interested in the feature maps to use the network as a feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "model.summary() # shows the various layers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test images\n",
    "\n",
    "Download two Mondrian paintings as test images and save to the current directory. We will try an abstract image and a landscape image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O landscape.jpg https://images.rkd.nl/rkd/thumb/650x650/bcb9558d-08a1-a57f-b5fc-ec562c446838.jpg\n",
    "!wget -O abstract.jpg https://images.rkd.nl/rkd/thumb/650x650/56c1a7ff-4661-12ea-e5bc-0f8be29c977a.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use the `keras.preprocessing.image` method `load_img` to read in the file, and display it with matplotlib. (Try with both the landscape and abstract image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_img(\"landscape.jpg\", target_size=(224, 224))\n",
    "# image = load_img(\"abstract.jpg\", target_size=(224, 224))\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert the image to a numpy array using the `keras.preprocessing.image` `img_to_array` method imported above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = img_to_array(image)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras expects an array of images when evaluating/predicting with the convolutional neural network. Let's reshape our single image into an array of images with only 1 member:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice this added an additional dimension to the front of the array. \n",
    "\n",
    "If you were extending this activity to calculate features for a whole set of images (say the mondrian paintings, or rothko, or you images from exercise 1), we would read in each of those images, and have all `n` of them stored in an `(n, 3, 224, 224)` array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Feature Vectors with VGG16\n",
    "\n",
    "In this step we will prepare an image to work with VGG16, using the `keras.applications.vgg16` `preprocess_input` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = preprocess_input(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the network in the \"forward\" direction. This means we provide the preprocessed image as an input and calculate the feature maps and activations for every layer in our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_feature = model.predict(image)\n",
    "vgg16_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our output (`vgg16_feature`) is an array of 512 7 x 7 maps. If we had passed in multiple test images to predict, we would see a result of shape `(n, 512, 7, 7)`, where each of `n` inputs has 512 7 x 7 feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the feature vectors as a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg16_feature_np = np.array(vgg16_feature)\n",
    "vgg16_feature_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Extracted Features as Feature Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display our extracted features as a grid of feature maps (using the raw `vgg_feature` directly). Here we will plot a subset of the 512 vectors (only 64 of them) in an 8 x 8 grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.title('First 64 Feature maps (7 x 7) For Image', fontsize=16);\n",
    "# plot 64 of the maps on an 8x8 square. (NOTE we have 512 total)\n",
    "xcount = 8\n",
    "ycount = 8\n",
    "ix = 1\n",
    "for _ in range(xcount):\n",
    "    for _ in range(ycount):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(xcount, ycount, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(vgg16_feature[0, ix-1, :, :], cmap='gray')\n",
    "        ix += 1\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: we are only displaying a subset (64) of the full 512 feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the Extracted Features as a Linear Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can flatten our feature maps into one linear vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_feature_vector = vgg16_feature_np.flatten()\n",
    "vgg16_feature_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our 512 7 x 7 feature maps gives us a 25088-dimensional (7 x 7 x 512= 25088) feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be easiest to display as a small grid. \n",
    "\n",
    "We can reshape our 1 x 25088 feature vector into a more manageable 392 x 64 array. We can display that as a small grid. Bright spots correspond to higher \"activations\" on the output feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4), dpi=150)\n",
    "plt.imshow(vgg16_feature_vector.reshape((64, 392)), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bright spots in this vector correspond to the degree of activation in the feature map outputs. Brighter spots correspond to the degree of \"activation\" of the high order features that the network has learned to detect through its training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "- Assemble a set of images (for instance mondrian paintings, or your images from exercise 1)\n",
    "- Calculate and store the VGG16 feature maps for each image\n",
    "- Clustering: use a clustering algorithm (k means, affinitey clustering, any others you know) to group the paintings according to their feature vectors. What groups do you see? Do they make sense?\n",
    "- Displaying: plot the results using UMAP, PCA, or t-SNE to see how our images are groupd in this high dimensional feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- Code for visualizing all layers: https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
